{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948b1fee-109e-4c85-8df0-b8287eae0db3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ab68e1-8bd8-4650-927d-4919776c8d2a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_path = Path('.') / 'api.env'\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70b17866-3849-4320-be78-56b7e5857fe5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "579c4655-7333-4a00-8ee3-1669b6aa21f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter company name:  microsoft\n",
      "Enter job title:  intern\n",
      "Enter role tag (optional):  swe intern\n",
      "Enter country (optional):  india\n",
      "Enter state (optional):  delhi \n",
      "How many LinkedIn profiles do you want to fetch? (e.g., 5):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for LinkedIn profiles...\n",
      "\n",
      "üìã Found 2 LinkedIn Profiles:\n",
      "1. https://in.linkedin.com/in/07ankitathakur\n",
      "2. https://in.linkedin.com/in/nishthapabreja\n",
      "Logging into LinkedIn...\n",
      "‚úÖ LinkedIn login successful\n",
      "\n",
      "üöÄ Starting to scrape 2 profiles...\n",
      "\n",
      "[1/2] Processing profile...\n",
      "Scraping: https://in.linkedin.com/in/07ankitathakur\n",
      "  üìã Extracting all sections for: Ankita .\n",
      "  üìä Found 16 sections\n",
      "  ‚úÖ Successfully scraped: Ankita . (16 sections, 5275 words)\n",
      "‚è≥ Waiting 5 seconds before next profile...\n",
      "\n",
      "[2/2] Processing profile...\n",
      "Scraping: https://in.linkedin.com/in/nishthapabreja\n",
      "  üìã Extracting all sections for: Nishtha Pabreja\n",
      "  üìä Found 13 sections\n",
      "  ‚úÖ Successfully scraped: Nishtha Pabreja (13 sections, 1340 words)\n",
      "üåê Browser closed after scraping.\n",
      "\n",
      "ü§ñ Starting AI processing for 2 profiles...\n",
      "ü§ñ AI Processing [1/2]: Ankita .\n",
      "  ‚úÖ AI processing completed for: Ankita .\n",
      "  ‚è≥ Waiting 2 seconds before next AI processing...\n",
      "ü§ñ AI Processing [2/2]: Nishtha Pabreja\n",
      "  ‚úÖ AI processing completed for: Nishtha Pabreja\n",
      "\n",
      "üíæ Saving results...\n",
      "\n",
      "‚úÖ Processing completed!\n",
      "üìÅ Raw scraped data saved to: raw_linkedin_profiles_microsoft_intern.json\n",
      "ü§ñ AI processed data saved to: ai_processed_profiles_microsoft_intern.json\n",
      "\n",
      "üìä PROCESSING SUMMARY:\n",
      "================================================================================\n",
      "üìà Overall Statistics:\n",
      "   ‚Ä¢ Total Profiles Scraped: 2\n",
      "   ‚Ä¢ AI Processing Successful: 2\n",
      "   ‚Ä¢ AI Processing Failed: 0\n",
      "   ‚Ä¢ Total Sections Extracted: 29\n",
      "   ‚Ä¢ Total Words Extracted: 6,615\n",
      "   ‚Ä¢ Average Sections per Profile: 14.5\n",
      "   ‚Ä¢ Average Words per Profile: 3308\n",
      "\n",
      "ü§ñ AI STRUCTURED PROFILES:\n",
      "================================================================================\n",
      "\n",
      "[Profile 1] Ankita .\n",
      "   Status: ‚úÖ Success\n",
      "   URL: https://in.linkedin.com/in/07ankitathakur\n",
      "   Raw Sections: 16\n",
      "   Raw Words: 5,275\n",
      "   AI Preview: Here is the parsed data according to the schema:  **Name:** Ankita .  **Headline:** Software Engineer Intern at Microsoft & Other Ventures  **About:** A passionate learner and problem solver with a strong foundation in computer science and engineering. Proficient in various programming languages inc...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Profile 2] Nishtha Pabreja\n",
      "   Status: ‚úÖ Success\n",
      "   URL: https://in.linkedin.com/in/nishthapabreja\n",
      "   Raw Sections: 13\n",
      "   Raw Words: 1,340\n",
      "   AI Preview: Here is the parsed data in the required schema:  **Name:** Nishtha Pabreja **Headline:** Software Engineer Intern at Microsoft | AI CLUB IGDTUW Intern | Manipal University Jaipur Alumnus **About:** Passionate about Machine Learning, Artificial Intelligence and Data Science. Strongly believe that tec...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Comprehensive structured report saved to: structured_report_microsoft_intern.txt\n",
      "\n",
      "üéâ All processing completed! Check the generated files for your structured LinkedIn data.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from serpapi import GoogleSearch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import (SystemMessagePromptTemplate, \n",
    "                                    HumanMessagePromptTemplate,\n",
    "                                    ChatPromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=Path('.') / 'api.env')\n",
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_KEY\")\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = \"llama3\"\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "def search_linkedin_profiles(company, job_title, role_tag=None, country=None, state=None, num_results=5):\n",
    "    \"\"\"Search for LinkedIn profiles using SerpAPI\"\"\"\n",
    "    location_filter = \"\"\n",
    "    if country:\n",
    "        location_filter += f' \"{country}\"'\n",
    "    if state:\n",
    "        location_filter += f' \"{state}\"'\n",
    "    \n",
    "    role_filter = \"\"\n",
    "    if role_tag:\n",
    "        role_filter = f' \"{role_tag}\"'\n",
    "    \n",
    "    query = f'{company} {job_title}{role_filter} site:linkedin.com/in/{location_filter}'\n",
    "    \n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"num\": num_results,\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "    }\n",
    "    \n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    links = []\n",
    "    if \"organic_results\" in results:\n",
    "        for result in results[\"organic_results\"]:\n",
    "            if \"link\" in result:\n",
    "                links.append(result[\"link\"])\n",
    "    \n",
    "    return links[:num_results]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing multiple newlines, tabs, and extra spaces\"\"\"\n",
    "    import re\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', '\\t', text)\n",
    "    text = re.sub(r'\\t\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s+', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_profile_with_ai(profile_data, profile_number, total_profiles):\n",
    "    \"\"\"Process individual profile using AI to extract structured information\"\"\"\n",
    "    try:\n",
    "        print(f\"ü§ñ AI Processing [{profile_number}/{total_profiles}]: {profile_data['name']}\")\n",
    "        \n",
    "        # Create system message\n",
    "        system_message = \"\"\"You are provided with LinkedIn profile data in JSON format.\n",
    "Parse the data according to the specified schema, correct any spelling errors,\n",
    "and condense the information if possible. Extract information accurately from the provided data.\"\"\"\n",
    "        \n",
    "        # Create human message with profile data\n",
    "        human_message = f\"\"\"### LinkedIn Profile JSON Data:\n",
    "{json.dumps(profile_data, indent=2)}\n",
    "\n",
    "### Schema You need to follow:\n",
    "You need to extract the following information in this exact format:\n",
    "\n",
    "Name:\n",
    "Headline:\n",
    "About:\n",
    "Experience:\n",
    "License:\n",
    "Education:\n",
    "Skills:\n",
    "Projects:\n",
    "Publications:\n",
    "Summary:\n",
    "\n",
    "Do not return preambles or any other information. Only return the structured data as requested.\n",
    "\n",
    "### Parsed Data:\"\"\"\n",
    "        \n",
    "        # Get AI response\n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=human_message)\n",
    "        ])\n",
    "        \n",
    "        ai_processed_data = {\n",
    "            'original_profile': profile_data,\n",
    "            'ai_structured_content': response.content,\n",
    "            'processing_status': 'success'\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ AI processing completed for: {profile_data['name']}\")\n",
    "        return ai_processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå AI processing failed for {profile_data['name']}: {str(e)}\")\n",
    "        return {\n",
    "            'original_profile': profile_data,\n",
    "            'ai_structured_content': f\"AI Processing Error: {str(e)}\",\n",
    "            'processing_status': 'error'\n",
    "        }\n",
    "\n",
    "def process_all_profiles_with_ai(all_profile_data):\n",
    "    \"\"\"Process all profiles using AI and return structured results\"\"\"\n",
    "    print(f\"\\nü§ñ Starting AI processing for {len(all_profile_data)} profiles...\")\n",
    "    \n",
    "    ai_processed_profiles = []\n",
    "    \n",
    "    for idx, profile in enumerate(all_profile_data, start=1):\n",
    "        # Process each profile individually\n",
    "        ai_result = process_profile_with_ai(profile, idx, len(all_profile_data))\n",
    "        ai_processed_profiles.append(ai_result)\n",
    "        \n",
    "        # Add delay between AI calls to avoid overwhelming the model\n",
    "        if idx < len(all_profile_data):\n",
    "            print(\"  ‚è≥ Waiting 2 seconds before next AI processing...\")\n",
    "            sleep(2)\n",
    "    \n",
    "    return ai_processed_profiles\n",
    "    \"\"\"Remove duplicate lines that are repeated within the same line\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        if len(line) > 1 and line[:len(line)//2] == line[len(line)//2:]:\n",
    "            new_lines.append(line[:len(line)//2])\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "    return '\\n'.join(new_lines)\n",
    "\n",
    "def scrape_linkedin_profile(driver, url):\n",
    "    \"\"\"Scrape comprehensive LinkedIn profile data including all sections\"\"\"\n",
    "    try:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        driver.get(url)\n",
    "        sleep(5)  # Wait longer for page to fully load\n",
    "        \n",
    "        profile_data = {}\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        \n",
    "        # Extract basic profile info\n",
    "        try:\n",
    "            name_element = soup.find(\"h1\", string=True)\n",
    "            if name_element:\n",
    "                name = name_element.get_text().strip()\n",
    "            else:\n",
    "                # Alternative selector for name\n",
    "                name_element = soup.find(\"h1\", {\"class\": \"text-heading-xlarge inline t-24 v-align-middle break-words\"})\n",
    "                name = name_element.get_text().strip() if name_element else \"Name not found\"\n",
    "        except:\n",
    "            name = \"Name not found\"\n",
    "        \n",
    "        try:\n",
    "            headline_element = soup.find('div', {'class': 'text-body-medium break-words'})\n",
    "            headline = headline_element.get_text().strip() if headline_element else \"Headline not found\"\n",
    "        except:\n",
    "            headline = \"Headline not found\"\n",
    "        \n",
    "        try:\n",
    "            location_element = soup.find('span', {'class': 'text-body-small inline t-black--light break-words'})\n",
    "            location = location_element.get_text().strip() if location_element else \"Location not found\"\n",
    "        except:\n",
    "            location = \"Location not found\"\n",
    "        \n",
    "        # Extract all sections from the profile\n",
    "        print(f\"  üìã Extracting all sections for: {name}\")\n",
    "        profile_main = soup.find('main')\n",
    "        sections = []\n",
    "        sections_data = []\n",
    "        \n",
    "        if profile_main:\n",
    "            sections = profile_main.find_all('section')\n",
    "            print(f\"  üìä Found {len(sections)} sections\")\n",
    "            \n",
    "            # Extract text from each section\n",
    "            sections_text = [section.get_text() for section in sections]\n",
    "            \n",
    "            # Clean the text from each section\n",
    "            sections_text = [clean_text(section) for section in sections_text]\n",
    "            sections_text = [remove_duplicates(section) for section in sections_text]\n",
    "            \n",
    "            # Create structured section data\n",
    "            for idx, section_text in enumerate(sections_text):\n",
    "                if section_text.strip():  # Only include non-empty sections\n",
    "                    sections_data.append({\n",
    "                        'section_number': idx + 1,\n",
    "                        'content': section_text,\n",
    "                        'word_count': len(section_text.split())\n",
    "                    })\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Main profile section not found!\")\n",
    "        \n",
    "        # Combine all section text for full profile content\n",
    "        full_profile_text = '\\n\\n'.join([section['content'] for section in sections_data])\n",
    "        \n",
    "        profile_data = {\n",
    "            'name': name,\n",
    "            'url': url,\n",
    "            'headline': headline,\n",
    "            'location': location,\n",
    "            'total_sections': len(sections_data),\n",
    "            'sections': sections_data,\n",
    "            'full_profile_text': full_profile_text,\n",
    "            'total_words': len(full_profile_text.split()) if full_profile_text else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ Successfully scraped: {name} ({len(sections_data)} sections, {profile_data['total_words']} words)\")\n",
    "        return profile_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error scraping {url}: {str(e)}\")\n",
    "        return {\n",
    "            'name': 'Error',\n",
    "            'url': url,\n",
    "            'headline': 'Error occurred',\n",
    "            'location': 'Error occurred',\n",
    "            'total_sections': 0,\n",
    "            'sections': [],\n",
    "            'full_profile_text': 'Error occurred',\n",
    "            'total_words': 0\n",
    "        }\n",
    "\n",
    "def setup_linkedin_driver():\n",
    "    \"\"\"Setup Chrome driver and login to LinkedIn\"\"\"\n",
    "    # Set up Chrome driver with options\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    # Login to LinkedIn\n",
    "    print(\"Logging into LinkedIn...\")\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    sleep(2)\n",
    "    \n",
    "    email_input = driver.find_element(By.ID, 'username')\n",
    "    email_input.send_keys(os.getenv(\"email\"))\n",
    "    \n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "    password_input.send_keys(os.getenv(\"password\"))\n",
    "    password_input.submit()\n",
    "    \n",
    "    sleep(5)  # Wait for login to complete\n",
    "    print(\"‚úÖ LinkedIn login successful\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def main():\n",
    "    # ‚úÖ Collect input from user\n",
    "    company = input(\"Enter company name: \").strip()\n",
    "    job_title = input(\"Enter job title: \").strip()\n",
    "    role_tag = input(\"Enter role tag (optional): \").strip()\n",
    "    country = input(\"Enter country (optional): \").strip()\n",
    "    state = input(\"Enter state (optional): \").strip()\n",
    "    \n",
    "    try:\n",
    "        num_results = int(input(\"How many LinkedIn profiles do you want to fetch? (e.g., 5): \").strip())\n",
    "    except ValueError:\n",
    "        num_results = 5  # fallback\n",
    "    \n",
    "    # üß† Make empty strings None\n",
    "    role_tag = role_tag or None\n",
    "    country = country or None\n",
    "    state = state or None\n",
    "    \n",
    "    # üîç Search LinkedIn profiles\n",
    "    print(\"\\nüîç Searching for LinkedIn profiles...\")\n",
    "    profile_links = search_linkedin_profiles(company, job_title, role_tag, country, state, num_results)\n",
    "    \n",
    "    if not profile_links:\n",
    "        print(\"‚ùå No LinkedIn profiles found!\")\n",
    "        return\n",
    "    \n",
    "    # üì§ Display found URLs\n",
    "    print(f\"\\nüìã Found {len(profile_links)} LinkedIn Profiles:\")\n",
    "    for idx, profile in enumerate(profile_links, start=1):\n",
    "        print(f\"{idx}. {profile}\")\n",
    "    \n",
    "    # ü§ñ Setup Selenium driver and login\n",
    "    driver = setup_linkedin_driver()\n",
    "    \n",
    "    # üìä Scrape all profiles\n",
    "    all_profile_data = []\n",
    "    print(f\"\\nüöÄ Starting to scrape {len(profile_links)} profiles...\")\n",
    "    \n",
    "    for idx, url in enumerate(profile_links, start=1):\n",
    "        print(f\"\\n[{idx}/{len(profile_links)}] Processing profile...\")\n",
    "        profile_data = scrape_linkedin_profile(driver, url)\n",
    "        all_profile_data.append(profile_data)\n",
    "        \n",
    "        # Add delay between requests to avoid being blocked\n",
    "        if idx < len(profile_links):\n",
    "            print(\"‚è≥ Waiting 5 seconds before next profile...\")\n",
    "            sleep(5)\n",
    "    \n",
    "    # Close the driver after scraping\n",
    "    driver.quit()\n",
    "    print(\"üåê Browser closed after scraping.\")\n",
    "    \n",
    "    # ü§ñ Process all profiles with AI\n",
    "    ai_processed_profiles = process_all_profiles_with_ai(all_profile_data)\n",
    "    \n",
    "    # üíæ Save results\n",
    "    print(\"\\nüíæ Saving results...\")\n",
    "    \n",
    "    # Save raw scraped data\n",
    "    raw_output_filename = f\"raw_linkedin_profiles_{company.replace(' ', '_')}_{job_title.replace(' ', '_')}.json\"\n",
    "    with open(raw_output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_profile_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save AI processed data\n",
    "    ai_output_filename = f\"ai_processed_profiles_{company.replace(' ', '_')}_{job_title.replace(' ', '_')}.json\"\n",
    "    with open(ai_output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ai_processed_profiles, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n‚úÖ Processing completed!\")\n",
    "    print(f\"üìÅ Raw scraped data saved to: {raw_output_filename}\")\n",
    "    print(f\"ü§ñ AI processed data saved to: {ai_output_filename}\")\n",
    "    \n",
    "    print(\"\\nüìä PROCESSING SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    successful_ai_processing = sum(1 for profile in ai_processed_profiles if profile['processing_status'] == 'success')\n",
    "    failed_ai_processing = len(ai_processed_profiles) - successful_ai_processing\n",
    "    \n",
    "    total_sections = sum(profile['total_sections'] for profile in all_profile_data)\n",
    "    total_words = sum(profile['total_words'] for profile in all_profile_data)\n",
    "    \n",
    "    print(f\"üìà Overall Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total Profiles Scraped: {len(all_profile_data)}\")\n",
    "    print(f\"   ‚Ä¢ AI Processing Successful: {successful_ai_processing}\")\n",
    "    print(f\"   ‚Ä¢ AI Processing Failed: {failed_ai_processing}\")\n",
    "    print(f\"   ‚Ä¢ Total Sections Extracted: {total_sections}\")\n",
    "    print(f\"   ‚Ä¢ Total Words Extracted: {total_words:,}\")\n",
    "    print(f\"   ‚Ä¢ Average Sections per Profile: {total_sections/len(all_profile_data):.1f}\")\n",
    "    print(f\"   ‚Ä¢ Average Words per Profile: {total_words/len(all_profile_data):.0f}\")\n",
    "    \n",
    "    print(f\"\\nü§ñ AI STRUCTURED PROFILES:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, ai_profile in enumerate(ai_processed_profiles, start=1):\n",
    "        original = ai_profile['original_profile']\n",
    "        print(f\"\\n[Profile {idx}] {original['name']}\")\n",
    "        print(f\"   Status: {'‚úÖ Success' if ai_profile['processing_status'] == 'success' else '‚ùå Failed'}\")\n",
    "        print(f\"   URL: {original['url']}\")\n",
    "        print(f\"   Raw Sections: {original['total_sections']}\")\n",
    "        print(f\"   Raw Words: {original['total_words']:,}\")\n",
    "        \n",
    "        if ai_profile['processing_status'] == 'success':\n",
    "            # Show a preview of the AI structured content\n",
    "            structured_preview = ai_profile['ai_structured_content'][:300].replace('\\n', ' ')\n",
    "            print(f\"   AI Preview: {structured_preview}{'...' if len(ai_profile['ai_structured_content']) > 300 else ''}\")\n",
    "        else:\n",
    "            print(f\"   Error: {ai_profile['ai_structured_content']}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Create comprehensive structured report\n",
    "    structured_report_filename = f\"structured_report_{company.replace(' ', '_')}_{job_title.replace(' ', '_')}.txt\"\n",
    "    with open(structured_report_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"AI STRUCTURED LINKEDIN PROFILE REPORT\\n\")\n",
    "        f.write(f\"Company: {company}\\n\")\n",
    "        f.write(f\"Job Title: {job_title}\\n\")\n",
    "        f.write(f\"Total Profiles: {len(ai_processed_profiles)}\\n\")\n",
    "        f.write(f\"Successful AI Processing: {successful_ai_processing}\\n\")\n",
    "        f.write(f\"Generated on: {os.popen('date').read().strip()}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for idx, ai_profile in enumerate(ai_processed_profiles, start=1):\n",
    "            original = ai_profile['original_profile']\n",
    "            f.write(f\"PROFILE {idx}: {original['name']}\\n\")\n",
    "            f.write(f\"URL: {original['url']}\\n\")\n",
    "            f.write(f\"Processing Status: {ai_profile['processing_status']}\\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\")\n",
    "            \n",
    "            if ai_profile['processing_status'] == 'success':\n",
    "                f.write(\"AI STRUCTURED DATA:\\n\")\n",
    "                f.write(ai_profile['ai_structured_content'])\n",
    "            else:\n",
    "                f.write(f\"PROCESSING ERROR:\\n{ai_profile['ai_structured_content']}\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Comprehensive structured report saved to: {structured_report_filename}\")\n",
    "    \n",
    "    # Close the driver\n",
    "    print(\"\\nüéâ All processing completed! Check the generated files for your structured LinkedIn data.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
